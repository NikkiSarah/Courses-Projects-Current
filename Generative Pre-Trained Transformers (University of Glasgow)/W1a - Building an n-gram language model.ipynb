{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building your own n-gram language model\n",
    "\n",
    "In this lab, you will build your own language model based on n-grams and the Maximum Likelihood Estimation.\n",
    "\n",
    "We'll assume that our text is already \"tokenized\" (split up into words). We'll cover this process in more depth in the next module.\n",
    "\n",
    "As an example, let's work with two sentences from \"[The Disappearance of Lady Frances Carfax](https://en.wikipedia.org/wiki/The_Disappearance_of_Lady_Frances_Carfax)\", a short story written by [Sir Arthur Conan Doyle](https://en.wikipedia.org/wiki/Arthur_Conan_Doyle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for the sentence \"It shows, my dear Watson, that we are dealing\n",
    "# with an exceptionally astude and dangerous man.\"\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "# Tokens for the sentence \"How would Lausanne do, my dear Watson?\"\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    " \n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Example:\n",
    "build_ngrams(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams(sample1, n=3)) == 17\n",
    "assert build_ngrams(sample1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert build_ngrams(sample1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(build_ngrams(sample1, n=2)) == 18\n",
    "assert build_ngrams(sample1, n=2)[0] == ('It', 'shows')\n",
    "assert build_ngrams(sample1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(build_ngrams(sample2, n=2)) == 8\n",
    "assert build_ngrams(sample2, n=2)[0] == ('How', 'would')\n",
    "assert build_ngrams(sample2, n=2)[7] == ('Watson', '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of your `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', '<BOS>', 'It'),\n",
       " ('<BOS>', 'It', 'shows'),\n",
       " ('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.'),\n",
       " ('man', '.', '<EOS>'),\n",
       " ('.', '<EOS>', '<EOS>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    # add beginning of sentence markers\n",
    "    tokens = ['<BOS>'] * (n - 1) + tokens\n",
    "    # add end of sentence markers\n",
    "    tokens = tokens + ['<EOS>'] * (n - 1)\n",
    "    # create the tuples\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    return ngrams\n",
    "\n",
    "# Example:\n",
    "build_ngrams_ctrl(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams_ctrl(sample1, n=3)) == 21\n",
    "assert build_ngrams_ctrl(sample1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample1, n=2)) == 20\n",
    "assert build_ngrams_ctrl(sample1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample2, n=2)) == 10\n",
    "assert build_ngrams_ctrl(sample2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert build_ngrams_ctrl(sample2, n=2)[9] == ('?', '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
       "             ('<BOS>', 'It'): {'shows': 1},\n",
       "             ('It', 'shows'): {',': 1},\n",
       "             ('shows', ','): {'my': 1},\n",
       "             (',', 'my'): {'dear': 2},\n",
       "             ('my', 'dear'): {'Watson': 2},\n",
       "             ('dear', 'Watson'): {',': 1, '?': 1},\n",
       "             ('Watson', ','): {'that': 1},\n",
       "             (',', 'that'): {'we': 1},\n",
       "             ('that', 'we'): {'are': 1},\n",
       "             ('we', 'are'): {'dealing': 1},\n",
       "             ('are', 'dealing'): {'with': 1},\n",
       "             ('dealing', 'with'): {'an': 1},\n",
       "             ('with', 'an'): {'exceptionally': 1},\n",
       "             ('an', 'exceptionally'): {'astute': 1},\n",
       "             ('exceptionally', 'astute'): {'and': 1},\n",
       "             ('astute', 'and'): {'dangerous': 1},\n",
       "             ('and', 'dangerous'): {'man': 1},\n",
       "             ('dangerous', 'man'): {'.': 1},\n",
       "             ('man', '.'): {'<EOS>': 1},\n",
       "             ('.', '<EOS>'): {'<EOS>': 1},\n",
       "             ('<BOS>', 'How'): {'would': 1},\n",
       "             ('How', 'would'): {'Lausanne': 1},\n",
       "             ('would', 'Lausanne'): {'do': 1},\n",
       "             ('Lausanne', 'do'): {',': 1},\n",
       "             ('do', ','): {'my': 1},\n",
       "             ('Watson', '?'): {'<EOS>': 1},\n",
       "             ('?', '<EOS>'): {'<EOS>': 1}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from collections import defaultdict\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    tuples = [build_ngrams_ctrl(text, n) for text in texts]\n",
    "    flat_tuples = [item for sublist in tuples for item in sublist]\n",
    "        \n",
    "    tuple_dict = defaultdict(dict)\n",
    "\n",
    "    for tup in flat_tuples:\n",
    "        key = tuple(tup[:-1])\n",
    "        value = tup[-1]\n",
    "\n",
    "        if value not in tuple_dict[key]:\n",
    "            tuple_dict[key][value] = 1\n",
    "        else:\n",
    "            tuple_dict[key][value] += 1\n",
    "    return tuple_dict\n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(count_ngrams([sample1, sample2], n=3)) == 28\n",
    "assert len(count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(count_ngrams([sample1, sample2], n=2)) == 24\n",
    "assert len(count_ngrams([sample1, sample2], n=2)['<BOS>',]) == 2\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['dear',]['Watson'] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       "             ('<BOS>', 'It'): {'shows': 1},\n",
       "             ('It', 'shows'): {',': 1},\n",
       "             ('shows', ','): {'my': 1},\n",
       "             (',', 'my'): {'dear': 1},\n",
       "             ('my', 'dear'): {'Watson': 1},\n",
       "             ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       "             ('Watson', ','): {'that': 1},\n",
       "             (',', 'that'): {'we': 1},\n",
       "             ('that', 'we'): {'are': 1},\n",
       "             ('we', 'are'): {'dealing': 1},\n",
       "             ('are', 'dealing'): {'with': 1},\n",
       "             ('dealing', 'with'): {'an': 1},\n",
       "             ('with', 'an'): {'exceptionally': 1},\n",
       "             ('an', 'exceptionally'): {'astute': 1},\n",
       "             ('exceptionally', 'astute'): {'and': 1},\n",
       "             ('astute', 'and'): {'dangerous': 1},\n",
       "             ('and', 'dangerous'): {'man': 1},\n",
       "             ('dangerous', 'man'): {'.': 1},\n",
       "             ('man', '.'): {'<EOS>': 1},\n",
       "             ('.', '<EOS>'): {'<EOS>': 1},\n",
       "             ('<BOS>', 'How'): {'would': 1},\n",
       "             ('How', 'would'): {'Lausanne': 1},\n",
       "             ('would', 'Lausanne'): {'do': 1},\n",
       "             ('Lausanne', 'do'): {',': 1},\n",
       "             ('do', ','): {'my': 1},\n",
       "             ('Watson', '?'): {'<EOS>': 1},\n",
       "             ('?', '<EOS>'): {'<EOS>': 1}})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    ngram_dict = count_ngrams(texts, n)\n",
    "    for key, sub_dict in ngram_dict.items():\n",
    "        num_words = len(sub_dict)\n",
    "        for key, values in sub_dict.items():\n",
    "            if num_words == 1:\n",
    "                sub_dict[key] = 1\n",
    "            else:\n",
    "                sub_dict[key] = values / num_words\n",
    "    return ngram_dict\n",
    "            \n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['dear',]['Watson'] == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "model = build_ngram_model(full_text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n",
      "\t\"\t36.9194\n",
      "\tThe\t1.3886\n",
      "\tHolmes\t1.0521\n",
      "\tI\t0.7630\n",
      "\tIt\t0.6445\n",
      "\t[206 more...]\n",
      "<BOS> It\n",
      "\twas\t7.0000\n",
      "\tis\t0.4375\n",
      "\tmay\t0.1875\n",
      "\thad\t0.1250\n",
      "\tseemed\t0.0625\n",
      "\t[11 more...]\n",
      "It was\n",
      "\ta\t0.6402\n",
      "\tthe\t0.2328\n",
      "\tnot\t0.1905\n",
      "\tonly\t0.1058\n",
      "\tan\t0.0847\n",
      "\t[184 more...]\n",
      "my dear\n",
      "\tWatson\t3.2353\n",
      "\tfellow\t0.8235\n",
      "\tsir\t0.5294\n",
      "\tyoung\t0.2941\n",
      "\tVon\t0.1176\n",
      "\t[12 more...]\n"
     ]
    }
   ],
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs)-5} more...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these probabilities look reasonable? How can we systematically evaluate the quality of our model? We'll cover this in the next lesson!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extra:\n",
    " - Smoothing techniques, such as [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), are often added to n-gram language models to deal with probabilities of 0. How might you modify your code to include smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
